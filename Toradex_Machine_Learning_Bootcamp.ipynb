{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmgz7IjOzxTk"
      },
      "source": [
        "# PyTorch Training Bootcamp\n",
        "## Toradex + Inova.USP\n",
        "\n",
        "18/11/2023\n",
        "\n",
        "Elaborado por: Bruno Mello (bruno.mello@toradex.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11rV1tvr0S-8"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKJNxxg70YiM"
      },
      "source": [
        "## Configuração do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il--DtPnOavv"
      },
      "source": [
        "#### Download do Dataset\n",
        "\n",
        "Neste exemplo estamos usando o dataset [Fruits and Vegetables Image Recognition Dataset](https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition), disponível sob a licensa [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/).\n",
        "\n",
        "Por estar disponível sob uma licensa de domínio público, podemos fazer basicamente o que quisermos com esse dataset, inclusive redistribuir ele.\n",
        "\n",
        "\n",
        "O dataset a ser usado na avaliação está disponível em:\n",
        "\n",
        "- Treinamento: https://docs.toradex.com/private/114105-recyclables_train.zip​\n",
        "- Validação: https://docs.toradex.com/private/114106-recyclables_validation.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R7nvkCrn3kF",
        "outputId": "08a272e3-fd20-410f-e933-ae2f2204ff2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlzQaFE-jZl6"
      },
      "outputs": [],
      "source": [
        "!# Baixar e descompactar o dataset\n",
        "!wget -q https://docs.toradex.com/private/114105-recyclables_train.zip​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOmYzYa7mxTG",
        "outputId": "a91bf3fd-425f-4728-d568-6cb90da92b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace dataset/train_crops/canister/canister/prepared_data_all_MGS-27-Oct_10-01-13_01.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q '/content/114105-recyclables_train.zip​' -d dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xwiML-eCkAVP"
      },
      "outputs": [],
      "source": [
        "# Baixar e descompactar o dataset\n",
        "!wget -q https://docs.toradex.com/private/114106-recyclables_validation.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD-fO9kPl5ge"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/114106-recyclables_validation.zip' -d dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM3q3JSer3Va"
      },
      "outputs": [],
      "source": [
        "data_path = \"dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_7E8wyiUyzh"
      },
      "source": [
        "### Organização de arquivos\n",
        "O menu lateral esquerdo pode ser usado para navegar sobre os arquivos.\n",
        "\n",
        "Por exemplo: /content/dataset/train/apple/Image_1.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yarLXoj6TB29"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(\"/content/dataset/validation_crops/bottle/bottle-blue/Monitoring_photo_2_test_25-Mar_11-13-25_01.jpg\", width=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSi2OpoUOkKG"
      },
      "source": [
        "#### Instalação e Carregamento de Dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNbPYYMX-ZxA"
      },
      "outputs": [],
      "source": [
        "# Carregar a extensão do TensorBoard para o Google Colab\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcnSCwmP00s9"
      },
      "outputs": [],
      "source": [
        "# Instalar Pytorch, Torchvision, Tensorboard e utilidades parar ver o progresso do treinamento\n",
        "!pip install -q torch torchvision torcheval tensorboard matplotlib tqdm tensorflow ipywidgets seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTgnGPSFPaRV"
      },
      "source": [
        "## Código Fonte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKf4hGeLPe4D"
      },
      "source": [
        "### Importação de dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCbB4XRKDwSG"
      },
      "outputs": [],
      "source": [
        "# Módulos, classes e funções que são úteis para inferência e treinamento\n",
        "import torch, torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torcheval.metrics import MulticlassF1Score, MulticlassRecall, MulticlassPrecision\n",
        "\n",
        "# Suporte a TensorBoard no PyTorch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Utiliades do sistema\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "# O Tqdm é utilizado para criar barras de progresso\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XULDjSaIPiiJ"
      },
      "source": [
        "### Dataset Customizado\n",
        "\n",
        "Implementamos um dataset customizado `CustomDataset`, que herda a classe `torch.utils.data.Dataset`. Essa nova classe foi feita para carregar um dataset organizado exatamente como o nosso está. Se o dataset estiver organizado de alguma outra forma, essa classe não será adequada para isso.\n",
        "\n",
        "Essa é a forma recomendada pela documentação do Pytorch de lidar com datasets, um pouco do background do porquê pode ser visto na página [Datasets & Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) da documentação.\n",
        "\n",
        "Também existem muitos [datasets disponíveis diretamente no PyTorch](https://pytorch.org/vision/stable/datasets.html), porém deve-se tomar cuidado com a licensa de uso para esses datasets, nem sempre uso comercial é permitido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYfh1MWrG_gy"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    # Construtor\n",
        "    def __init__(self, images_dir, preprocess_function):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images_dir (string): Directory with all the image folders\n",
        "            preprocess_function (callable): Transform to be applied on a sample\n",
        "        \"\"\"\n",
        "\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = preprocess_function\n",
        "\n",
        "        # Ordenamos as classes para termos sempre a mesma ordem\n",
        "        # Aqui ficam armazenados os nomes de cada classe\n",
        "        self.classes = []\n",
        "        pasta = os.listdir(self.images_dir)\n",
        "        self.super_classes = sorted(os.listdir(self.images_dir))\n",
        "\n",
        "        # Caminho para cada imagem\n",
        "        self.image_paths = []\n",
        "        self.image_classes = []\n",
        "\n",
        "        contador = -1\n",
        "        # Procura imagens para todas as classes, em suas respectivas pastas\n",
        "        for i in range(0, len(self.super_classes)):\n",
        "          sample_class = self.super_classes[i]\n",
        "          class_dir = os.path.join(self.images_dir, sample_class)\n",
        "          class_images = sorted(os.listdir(class_dir))\n",
        "\n",
        "          for sub_pastas in class_images:\n",
        "            caminho = os.path.join(class_dir, sub_pastas)\n",
        "            nova_lista = os.listdir(caminho)\n",
        "            contador = contador + 1\n",
        "            self.classes.append(sub_pastas)\n",
        "\n",
        "            for image in nova_lista:\n",
        "              if(not (image.endswith(\".jpg\") or image.endswith(\".JPG\") or image.endswith(\".png\") or image.endswith(\".PNG\"))):\n",
        "                  continue\n",
        "              image_path = os.path.join(class_dir, sub_pastas, image)\n",
        "              self.image_paths.append(image_path)\n",
        "              self.image_classes.append(contador)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Checagem por erros na leitura da imagem\n",
        "        # Isso gera uma nova excessão para que o dataset seja corrigido, não queremos imagens com problemas no dataset\n",
        "        try:\n",
        "            sample = read_image(self.image_paths[idx], torchvision.io.ImageReadMode.RGB)\n",
        "        except:\n",
        "            print(f\"Problem loading image {self.image_paths[idx]}\")\n",
        "            raise Exception()\n",
        "\n",
        "        # Preprocessamento da imagem\n",
        "        sample = self.transform(sample)\n",
        "\n",
        "        # Retorno do par (imagem, classe)\n",
        "        return sample, self.image_classes[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oobk1RwqVxtv"
      },
      "source": [
        "### Pré-processamento\n",
        "\n",
        "Na etapa de pré-processamento, deve-se formatar os dados ao formato esperado pelo modelo utilizado.\n",
        "\n",
        "Aqui, a imagem é primeiro redimensionada para a resolução de `inference_size`x`inference_size`, depois convertida para ponto flutuante (float) e depois normalizada. Os valores de normalização `NormalizationMean` e `NormalizationStd` são dados pelo modelo utilizado. No caso ResNet, disponível no [PyTorch Hub](https://pytorch.org/hub/pytorch_vision_resnet/). O uso dos `transforms` do PyTorch facilita o preprocessamento de dados.\n",
        "\n",
        "O `batch_size` representa o número de imagens que será processada de uma vez. Isso é útil para atingir melhor paralelismo na inferência e no treinamento do modelo utilizado, mas tem um impacto no tempo de inferência e no uso de memória RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJZ54s-A6NBZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "inference_size = 160\n",
        "NormalizationMean = [0.485, 0.456, 0.406]\n",
        "NormalizationStd = [0.229, 0.224, 0.225]\n",
        "\n",
        "preprocess_image = transforms.Compose([\n",
        "    transforms.Resize((inference_size, inference_size), antialias=True),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize(mean=NormalizationMean, std=NormalizationStd),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NZ-rq5YX4hn"
      },
      "source": [
        "### Instanciamento dos datasets\n",
        "\n",
        "Inicialmente definimos o caminho em que os datasets estão, relativos ao `data_path` que definimos quando baixamos e descompactamos o dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzA10BCoVSiM"
      },
      "outputs": [],
      "source": [
        "train_data_path = os.path.join(data_path, \"train_crops\")\n",
        "validation_data_path = os.path.join(data_path, \"validation_crops\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "700F01Zgs1VC"
      },
      "outputs": [],
      "source": [
        "test_data_path = os.path.join(data_path, \"test_crops\") #ARRUMAR AQUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kh7LMldavRo"
      },
      "source": [
        "Agora, inicializamos as classes `CustomDataset` e `DataLoader` para os datasets que temos:\n",
        "\n",
        "- Treinamento: Grande dataset, será usado para ajustar os pesos do modelo\n",
        "- Validação: Menor dataset, será usado para avaliar o modelo treinado e escolher o checkpoint com melhor performance\n",
        "- Teste: Menor dataset, será usado para medir a performance do checkpoint escolhido durante o treinamento\n",
        "\n",
        "É importante ressaltar que os datasets não devem ter imagens em comum entre si."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feD6vB0SHZdb"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_data_path, preprocess_image)\n",
        "validation_dataset = CustomDataset(validation_data_path, preprocess_image)\n",
        "# test_dataset = CustomDataset(test_data_path, preprocess_image)\n",
        "\n",
        "print(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "print( \"Train dataset information:\")\n",
        "print(f\" |-> Path:          {train_data_path}\")\n",
        "print(f\" |-> Length:        {len(train_dataset)}\")\n",
        "print(f\" '-> Loader length: {len(train_dataloader)}\")\n",
        "\n",
        "print( \"Validation dataset information:\")\n",
        "print(f\" |-> Path:          {validation_data_path}\")\n",
        "print(f\" |-> Length:        {len(validation_dataset)}\")\n",
        "print(f\" '-> Loader length: {len(validation_dataloader)}\")\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjAPdPjbe4QI"
      },
      "source": [
        "### Carregar o modelo\n",
        "\n",
        "Existem diferentes tamanhos para o modelo resnet, quanto maior o modelo, mais preciso ele consegue ficar, mas ao mesmo tempo, maior é o tempo de execução.\n",
        "\n",
        "A resolução da imagem usada na inferência também tem influência no tempo de execução, em um sistema embarcado muitas vezes é necessário avaliar a melhor combinação de resolução e tamanho de modelo para ter uma performance aceitável, tanto em termos de qualidade da saída quanto em termos de uso de recursos computacionais limitados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGTLogsX8Zdu"
      },
      "outputs": [],
      "source": [
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', weights=torchvision.models.ResNet34_Weights.DEFAULT)\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', weights=torchvision.models.ResNet101_Weights.DEFAULT)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', weights=torchvision.models.ResNet152_Weights.DEFAULT)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi2_zy34ZK5z"
      },
      "source": [
        "### TensorBoard\n",
        "\n",
        "O TensorBoard é utilizado para visualizar estatísticas de treinamento e acompanhar o progresso em tempo real.\n",
        "\n",
        "No Google Colab, ele deve ser inicializado antes de se iniciar o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXJAr6H19D0"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuxFgjBzxVzY"
      },
      "source": [
        "### Função para treinar o modelo por uma época\n",
        "\n",
        "Treinar o modelo é, de forma simplificada, otimizar os parâmetros da grande equação que é o modelo para que os resultados sejam mais próximos do esperado. Isso é feito usando uma heurística de perda (loss), que é minimizada.\n",
        "\n",
        "Uma época neste caso é definida como uma iteração do treinamento para cada amostra do dataset de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvy6nRFQHZS1"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_function, epoch_index, device, tensorboard_writer):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    print(f\"Training Epoch {epoch_index}:\")\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader))\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero gradients for every batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run Inference on the training data\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            last_loss = running_loss / 10 # loss per batch\n",
        "            tb_x = (epoch_index*len(train_dataloader)) + i\n",
        "            tensorboard_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            tensorboard_writer.flush()\n",
        "            running_loss = 0.0\n",
        "\n",
        "        del inputs, labels\n",
        "\n",
        "        progress_bar.update(1)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h74LelA_o4L"
      },
      "outputs": [],
      "source": [
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "tensorboard_writer = SummaryWriter(f'runs/model_{timestamp}')\n",
        "\n",
        "EPOCHS = 6\n",
        "\n",
        "best_validation_loss = -1\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.008, momentum=0.9)\n",
        "\n",
        "device = 'cpu'\n",
        "if(torch.cuda.is_available()):\n",
        "    device = 'cuda'\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'EPOCH {epoch}:')\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(model, train_dataloader, optimizer, loss_function, epoch, device, tensorboard_writer)\n",
        "\n",
        "    running_validation_loss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    precision_metric = MulticlassPrecision(average='weighted', num_classes=len(validation_dataset.classes))\n",
        "    recall_metric = MulticlassRecall(average='weighted', num_classes=len(validation_dataset.classes))\n",
        "    f1score_metric = MulticlassF1Score(average='weighted', num_classes=len(validation_dataset.classes))\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    print(f\"Running Validation for Epoch {epoch}\")\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(total=len(validation_dataloader))\n",
        "        for validation_data in validation_dataloader:\n",
        "            validation_inputs, validation_labels = validation_data\n",
        "            validation_inputs = validation_inputs.to(device)\n",
        "            validation_labels = validation_labels.to(device)\n",
        "            validation_outputs = model(validation_inputs)\n",
        "            running_validation_loss += loss_function(validation_outputs, validation_labels)\n",
        "\n",
        "            output_labels = []\n",
        "\n",
        "            for j in range(0, validation_outputs.size()[0]):\n",
        "                probabilities = torch.nn.functional.softmax(validation_outputs[j], dim=0)\n",
        "                prob, det_class = torch.topk(probabilities, 1)\n",
        "                # If the detected class is outside of bounds, give a wrong result within bounds\n",
        "                if(det_class >= len(validation_dataset.classes)):\n",
        "                    det_class = validation_labels[j].item()-1\n",
        "                    if(det_class < 0):\n",
        "                        det_class = validation_labels[j].item()+1\n",
        "                output_labels.append(det_class)\n",
        "\n",
        "            output_labels = torch.as_tensor(output_labels)\n",
        "            output_labels.to(device)\n",
        "\n",
        "            precision_metric.update(output_labels, validation_labels)\n",
        "            recall_metric.update(output_labels, validation_labels)\n",
        "            f1score_metric.update(output_labels, validation_labels)\n",
        "\n",
        "            del validation_inputs, validation_labels, output_labels\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    precision = precision_metric.compute().item()\n",
        "    recall = recall_metric.compute().item()\n",
        "    f1score = f1score_metric.compute().item()\n",
        "\n",
        "    average_validation_loss = running_validation_loss / len(validation_dataloader)\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    tensorboard_writer.add_scalars('Training vs. Validation Loss',\n",
        "                                  { 'Training': avg_loss,\n",
        "                                    'Validation': average_validation_loss},\n",
        "                                    epoch)\n",
        "\n",
        "    tensorboard_writer.add_scalars('Validation Metrics',\n",
        "                                  { 'Precision': precision,\n",
        "                                    'Recall': recall,\n",
        "                                    'F1 Score': f1score},\n",
        "                                    epoch)\n",
        "    tensorboard_writer.flush()\n",
        "\n",
        "    tensorboard_writer.add_scalars('Learning Rate',\n",
        "                                  {'lr': optimizer.state_dict()['param_groups'][0]['lr']},\n",
        "                                    epoch)\n",
        "    tensorboard_writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if((average_validation_loss < best_validation_loss) or (best_validation_loss == -1)):\n",
        "        best_validation_loss = average_validation_loss\n",
        "        torch.save(model.state_dict(), f'model_{timestamp}_best')\n",
        "\n",
        "    torch.save(model.state_dict(), f'model_{timestamp}_{epoch}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4V3yXRHU2Pr"
      },
      "source": [
        "### Teste do modelo\n",
        "\n",
        "A Função abaixo serve para testar o modelo, calculando uma matriz de confusão, precisão, recall e F1 Score.\n",
        "A Função também mede o tempo de execução, mas isso não é reproduzível o suficiente no Google Colab.\n",
        "\n",
        "Notebook do Google Colab com ajustes esperados: https://colab.research.google.com/drive/1c5JcE1FtDuoL8bnZC7ST9vsveBgDqehS?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwQai1XLU64M"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from torcheval.metrics import MulticlassConfusionMatrix\n",
        "\n",
        "def test_model(model, device, test_dataset, batch_size=1, run_tests=True, time_test_iterations=1000, show_confusion_matrix=True, test_time=True):\n",
        "\n",
        "    if(run_tests and show_confusion_matrix):\n",
        "        confusion_matrix_metric = MulticlassConfusionMatrix(num_classes=len(test_dataset.classes))\n",
        "\n",
        "    if(run_tests):\n",
        "        precision_metric = MulticlassPrecision(average='weighted', num_classes=len(test_dataset.classes))\n",
        "        recall_metric = MulticlassRecall(average='weighted', num_classes=len(test_dataset.classes))\n",
        "        f1score_metric = MulticlassF1Score(average='weighted', num_classes=len(test_dataset.classes))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "    if(run_tests):\n",
        "        progress_bar = tqdm(total=len(test_dataloader))\n",
        "        for test_data in test_dataloader:\n",
        "            test_inputs, test_labels = test_data\n",
        "            test_inputs = test_inputs.to(device)\n",
        "            test_labels = test_labels.to(device)\n",
        "            test_outputs = model(test_inputs)\n",
        "            output_labels = []\n",
        "\n",
        "            for j in range(0, test_outputs.size()[0]):\n",
        "                probabilities = torch.nn.functional.softmax(test_outputs[j], dim=0)\n",
        "                prob, det_class = torch.topk(probabilities, 1)\n",
        "                # If the detected class is outside of bounds, give a wrong result within bounds\n",
        "                if(det_class >= len(test_dataset.classes)):\n",
        "                    det_class = test_labels[j].item()-1\n",
        "                    if(det_class < 0):\n",
        "                        det_class = test_labels[j].item()+1\n",
        "                output_labels.append(det_class)\n",
        "\n",
        "            output_labels = torch.as_tensor(output_labels)\n",
        "            output_labels.to(device)\n",
        "\n",
        "            confusion_matrix_metric.update(output_labels, test_labels)\n",
        "            precision_metric.update(output_labels, test_labels)\n",
        "            recall_metric.update(output_labels, test_labels)\n",
        "            f1score_metric.update(output_labels, test_labels)\n",
        "\n",
        "            del test_inputs, test_labels\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        precision = precision_metric.compute().item()\n",
        "        recall = recall_metric.compute().item()\n",
        "        f1score = f1score_metric.compute().item()\n",
        "\n",
        "        if(show_confusion_matrix):\n",
        "            confusion_matrix_dataframe = pd.DataFrame(confusion_matrix_metric.normalized(),\n",
        "                                                      index = test_dataset.classes,\n",
        "                                                      columns = test_dataset.classes)\n",
        "            plt.figure(figsize = (16,12))\n",
        "            ax = sn.heatmap(confusion_matrix_dataframe, annot=True, cmap = 'YlOrBr')\n",
        "            ax.xaxis.tick_top()\n",
        "            ax.set_xticks(range(1, len(test_dataset.classes)+1), test_dataset.classes, rotation=270, ha='right');\n",
        "\n",
        "    else:\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        f1score = 0.0\n",
        "\n",
        "\n",
        "    # Como o Colab possui um tempo de execução muito variável,\n",
        "    # o tempo pode ser definido como constante, usando um dos valores testados.\n",
        "    average_time = 0.00565 # Resnet 50, 160x160\n",
        "\n",
        "    if(test_time):\n",
        "        model_input, _ = next(iter(test_dataloader))\n",
        "        model_input = model_input.to(device)\n",
        "\n",
        "        total_time = 0\n",
        "        for i in range(time_test_iterations):\n",
        "            t0 = datetime.now()\n",
        "            outputs = model(model_input)\n",
        "            t1 = datetime.now()\n",
        "            total_time += (t1-t0).total_seconds()\n",
        "\n",
        "        del model_input\n",
        "\n",
        "        average_time = total_time/time_test_iterations\n",
        "\n",
        "    final_score = 10*f1score*math.pow(1-average_time, 2)\n",
        "\n",
        "    return final_score, {\"average_time\": average_time, \"f1score\": f1score, \"precision\": precision, \"recall\": recall}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVMP5i5GU-Fh"
      },
      "outputs": [],
      "source": [
        "best_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50')\n",
        "best_model.load_state_dict(torch.load(f'model_{timestamp}_best'))\n",
        "\n",
        "test_results = test_model(model=best_model,\n",
        "                          device=device,\n",
        "                          test_dataset=test_dataset,\n",
        "                          batch_size=1,\n",
        "                          run_tests=True,\n",
        "                          time_test_iterations=1000,\n",
        "                          show_confusion_matrix=True)\n",
        "\n",
        "print(f\"Test data for model:\")\n",
        "print(f\"  '-> Final Score: {test_results[0]}\")\n",
        "print(f\"      |-> Average Time: {test_results[1]['average_time']}\")\n",
        "print(f\"      '-> F1 Score:     {test_results[1]['f1score']}\")\n",
        "print(f\"          |-> Precision: {test_results[1]['precision']}\")\n",
        "print(f\"          '-> Recall:    {test_results[1]['recall']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGpsdwMIYtY"
      },
      "source": [
        "Ordem Esperada para as classes no dataset de recicláveis:\n",
        "\n",
        "- Class 0: bottle-blue\n",
        "- Class 1: bottle-blue-full\n",
        "- Class 2: bottle-blue5l\n",
        "- Class 3: bottle-blue5l-full\n",
        "- Class 4: bottle-dark\n",
        "- Class 5: bottle-dark-full\n",
        "- Class 6: bottle-green\n",
        "- Class 7: bottle-green-full\n",
        "- Class 8: bottle-milk\n",
        "- Class 9: bottle-milk-full\n",
        "- Class 10: bottle-multicolor\n",
        "- Class 11: bottle-multicolorv-full\n",
        "- Class 12: bottle-oil\n",
        "- Class 13: bottle-oil-full\n",
        "- Class 14: bottle-transp\n",
        "- Class 15: bottle-transp-full\n",
        "- Class 16: bottle-yogurt\n",
        "- Class 17: glass-dark\n",
        "- Class 18: glass-green\n",
        "- Class 19: glass-transp\n",
        "- Class 20: canister\n",
        "- Class 21: cans\n",
        "- Class 22: juice-cardboard\n",
        "- Class 23: milk-cardboard\n",
        "- Class 24: detergent-box\n",
        "- Class 25: detergent-color\n",
        "- Class 26: detergent-transparent\n",
        "- Class 27: detergent-white"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}